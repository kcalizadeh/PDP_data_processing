{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v_for_imports.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fRqjeaLlmZHe",
        "_7J7_WOz2VK8"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPNWMNEmHwmDWrz4jyO7il3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcalizadeh/PDP_data_processing/blob/master/w2v_for_imports.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxG_6U1lGEa"
      },
      "source": [
        "### Imports and Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRT6BjwbiM8n",
        "outputId": "025a6c2f-1088-4538-c89d-5bd07a17a7a2"
      },
      "source": [
        "# this cell mounts drive, sets the correct directory, then imports all functions\n",
        "# and relevant libraries via the functions.py file\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/gdrive',force_remount=True)\n",
        "\n",
        "drive_path = '/gdrive/MyDrive/Colab_Projects/Phil_NLP'\n",
        "\n",
        "sys.path.append(drive_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_FSrsN8l0J4"
      },
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "\n",
        "# a function for quickly testing w2v models\n",
        "def test_w2v(model, pairs):\n",
        "  for (pos, neg) in pairs:\n",
        "    math_result = model.most_similar(positive=pos, negative=neg)\n",
        "    print(f'Positive - {pos}\\tNegative - {neg}')\n",
        "    [print(f\"- {result[0]} ({round(result[1],5)})\") for result in math_result[:5]]\n",
        "    print()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRqjeaLlmZHe"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "PHxy7346l3hZ",
        "outputId": "6c77f4e1-3253-4b5a-be6e-4673671baeb7"
      },
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/phil_nlp.csv')\n",
        "\n",
        "df.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>school</th>\n",
              "      <th>sentence_spacy</th>\n",
              "      <th>sentence_str</th>\n",
              "      <th>sentence_length</th>\n",
              "      <th>sentence_lowered</th>\n",
              "      <th>tokenized_txt</th>\n",
              "      <th>lemmatized_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>78501</th>\n",
              "      <td>Aristotle - Complete Works</td>\n",
              "      <td>Aristotle</td>\n",
              "      <td>aristotle</td>\n",
              "      <td>And those who are in a position of superiority...</td>\n",
              "      <td>And those who are in a position of superiority...</td>\n",
              "      <td>188</td>\n",
              "      <td>and those who are in a position of superiority...</td>\n",
              "      <td>['and', 'those', 'who', 'are', 'in', 'position...</td>\n",
              "      <td>and those who be in a position of superiority...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178416</th>\n",
              "      <td>Philosophical Troubles</td>\n",
              "      <td>Kripke</td>\n",
              "      <td>analytic</td>\n",
              "      <td>This shows that, normally, if the third condit...</td>\n",
              "      <td>This shows that, normally, if the third condit...</td>\n",
              "      <td>74</td>\n",
              "      <td>this shows that, normally, if the third condit...</td>\n",
              "      <td>['this', 'shows', 'that', 'normally', 'if', 't...</td>\n",
              "      <td>this show that , normally , if the third cond...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283301</th>\n",
              "      <td>Elements Of The Philosophy Of Right</td>\n",
              "      <td>Hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>that is, Ideas in general, and hence also the ...</td>\n",
              "      <td>that is, Ideas in general, and hence also the ...</td>\n",
              "      <td>234</td>\n",
              "      <td>that is, ideas in general, and hence also the ...</td>\n",
              "      <td>['that', 'is', 'ideas', 'in', 'general', 'and'...</td>\n",
              "      <td>that is , Ideas in general , and hence also t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65488</th>\n",
              "      <td>Aristotle - Complete Works</td>\n",
              "      <td>Aristotle</td>\n",
              "      <td>aristotle</td>\n",
              "      <td>Now their bite is not rough and fiery, but mal...</td>\n",
              "      <td>Now their bite is not rough and fiery, but mal...</td>\n",
              "      <td>53</td>\n",
              "      <td>now their bite is not rough and fiery, but mal...</td>\n",
              "      <td>['now', 'their', 'bite', 'is', 'not', 'rough',...</td>\n",
              "      <td>now -PRON- bite be not rough and fiery , but ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85882</th>\n",
              "      <td>Aristotle - Complete Works</td>\n",
              "      <td>Aristotle</td>\n",
              "      <td>aristotle</td>\n",
              "      <td>In addition stringent laws must be laid down f...</td>\n",
              "      <td>In addition stringent laws must be laid down f...</td>\n",
              "      <td>232</td>\n",
              "      <td>in addition stringent laws must be laid down f...</td>\n",
              "      <td>['in', 'addition', 'stringent', 'laws', 'must'...</td>\n",
              "      <td>in addition stringent law must be lay down fo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      title  ...                                     lemmatized_str\n",
              "78501            Aristotle - Complete Works  ...   and those who be in a position of superiority...\n",
              "178416               Philosophical Troubles  ...   this show that , normally , if the third cond...\n",
              "283301  Elements Of The Philosophy Of Right  ...   that is , Ideas in general , and hence also t...\n",
              "65488            Aristotle - Complete Works  ...   now -PRON- bite be not rough and fiery , but ...\n",
              "85882            Aristotle - Complete Works  ...   in addition stringent law must be lay down fo...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR-d0hRI7hmQ"
      },
      "source": [
        "# using gensim's built-in tokenizer \r\n",
        "df['gensim_tokenized'] = df['sentence_str'].map(lambda x: simple_preprocess(x.lower(),deacc=True,\r\n",
        "                                                        max_len=100))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-mTBCSDojS1",
        "outputId": "7b489098-16e3-4699-9800-1b49c1bac094"
      },
      "source": [
        "# check how it worked\r\n",
        "print(df.iloc[290646]['sentence_str'])\r\n",
        "df['gensim_tokenized'][290646]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A spider conducts operations that resemble those of a weaver, and a bee puts to shame many an architect in the construction of her cells.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spider',\n",
              " 'conducts',\n",
              " 'operations',\n",
              " 'that',\n",
              " 'resemble',\n",
              " 'those',\n",
              " 'of',\n",
              " 'weaver',\n",
              " 'and',\n",
              " 'bee',\n",
              " 'puts',\n",
              " 'to',\n",
              " 'shame',\n",
              " 'many',\n",
              " 'an',\n",
              " 'architect',\n",
              " 'in',\n",
              " 'the',\n",
              " 'construction',\n",
              " 'of',\n",
              " 'her',\n",
              " 'cells']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KxV2JTIoyxE"
      },
      "source": [
        "Hmm, an interesting observation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5ZnODM2V_P"
      },
      "source": [
        "### Transfer Learning with GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAOaelq2WIr"
      },
      "source": [
        "We'll import GloVe vectors as w2v, then use those as a base from which to train new vectors that are tuned to our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcFZMRT82WRC"
      },
      "source": [
        "# load the vectors. other vector sizes were used but yielded generally less sensible models\r\n",
        "glove_file = datapath('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/glove.6B.50d.txt')\r\n",
        "tmp_file = get_tmpfile(\"test_word2vec.txt\")\r\n",
        "\r\n",
        "_ = glove2word2vec(glove_file, tmp_file)\r\n",
        "\r\n",
        "glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hPyZ2aHugg"
      },
      "source": [
        "pairs_to_try = [(['law', 'moral'], []),\r\n",
        "                (['self', 'consciousness'], []),\r\n",
        "                (['dialectic'], []),\r\n",
        "                (['logic'], []),\r\n",
        "]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjdDWtd72Tli",
        "outputId": "6f7a0444-0950-4926-da58-43da7a34e149"
      },
      "source": [
        "# check out how GloVe works on our test pairs\r\n",
        "test_w2v(glove_vectors, pairs_to_try)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- morality (0.82654)\n",
            "- legal (0.82652)\n",
            "- laws (0.81529)\n",
            "- constitutional (0.80616)\n",
            "- fundamental (0.80217)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- sense (0.83446)\n",
            "- mind (0.79755)\n",
            "- vision (0.78202)\n",
            "- belief (0.78031)\n",
            "- life (0.77984)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- hegelian (0.88376)\n",
            "- dialectical (0.83417)\n",
            "- dialectics (0.80672)\n",
            "- materialist (0.77674)\n",
            "- metaphysics (0.77488)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- reasoning (0.81405)\n",
            "- intuitionistic (0.76531)\n",
            "- concepts (0.75831)\n",
            "- logical (0.75604)\n",
            "- theory (0.75026)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFyauZoo7KW"
      },
      "source": [
        "Now we want these to be trained on our actual philosophical texts - that way we can see how different thinkers use different words and potentially use the vectors for classification.\r\n",
        "\r\n",
        "So in the cells below we train the existing GloVe model on on the German Idealist texts as a test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVDl6FwMmwp9"
      },
      "source": [
        "#### German Idealism Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSOA8Zz2o7vX"
      },
      "source": [
        "# isolate the relevant school\r\n",
        "documents = df[df['school'] == 'german_idealism']['gensim_tokenized']\r\n",
        "\r\n",
        "# format the series to be used\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "# get bigrams\r\n",
        "bigram = Phrases(cleaned, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "bigramed_tokens = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens = bigram_phraser[sent]\r\n",
        "    bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# run again to get trigrams\r\n",
        "trigram = Phrases(bigramed_tokens, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "trigramed_tokens = []\r\n",
        "for sent in bigramed_tokens:\r\n",
        "    tokens = trigram_phraser[sent]\r\n",
        "    trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "base_model = Word2Vec(size=300, min_count=5)\r\n",
        "base_model.build_vocab(trigramed_tokens)\r\n",
        "total_examples = base_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "base_model.train(trigramed_tokens, total_examples=total_examples, epochs=base_model.epochs)\r\n",
        "base_model_wv = base_model.wv\r\n",
        "del base_model"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0FjXahncQya",
        "outputId": "1590a919-27eb-4a43-dab8-5cad6ed54b0e"
      },
      "source": [
        "test_w2v(base_model_wv, pairs_to_try)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- freedom (0.82856)\n",
            "- rule (0.80684)\n",
            "- morality (0.80089)\n",
            "- moral law (0.7894)\n",
            "- highest good (0.78798)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- self consciousness (0.91867)\n",
            "- essence (0.86718)\n",
            "- objectivity (0.86468)\n",
            "- negativity (0.86383)\n",
            "- positedness (0.86014)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- deduction (0.90418)\n",
            "- antinomy (0.89887)\n",
            "- exposition (0.89578)\n",
            "- method (0.8956)\n",
            "- definition (0.89019)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- science (0.84214)\n",
            "- pure reason (0.83874)\n",
            "- idealism (0.83832)\n",
            "- doctrine (0.83681)\n",
            "- metaphysics (0.81759)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR3MnHFbcl7d"
      },
      "source": [
        "These seem to reflect the true uses of words in German Idealism. 'Self' + 'consciousness' is rightly associated with 'self consciousness' and 'moral' + 'law' with 'moral law'. It even identifies the German Idealist tendency to unify logic and metaphysics. \r\n",
        "\r\n",
        "These vectors can be fairly said to reflect how german idealists use these terms. Moreover, they are significantly different than the original GloVe model, which indicates that there was real learning going on here.\r\n",
        "\r\n",
        "For comparison, let's check these same terms, but as used by Phenomenologists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPdzhIk0m0Qz"
      },
      "source": [
        "#### Phenomenology Comparision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s1Jkkc7dqk9"
      },
      "source": [
        "def train_glove(source_type, source, glove_vectors, threshold=10, stopwords=[],\r\n",
        "                min_count=20):\r\n",
        "  # isolate the relevant school\r\n",
        "  documents = df[df[source_type] == source]['gensim_tokenized']\r\n",
        "\r\n",
        "  # format the series to be used\r\n",
        "  stopwords = []\r\n",
        "\r\n",
        "  sentences = [sentence for sentence in documents]\r\n",
        "  cleaned = []\r\n",
        "  for sentence in sentences:\r\n",
        "    cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "    cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "    cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "  # get bigrams\r\n",
        "  bigram = Phrases(cleaned, min_count=min_count, threshold=threshold, \r\n",
        "                   delimiter=b' ')\r\n",
        "  bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "  bigramed_tokens = []\r\n",
        "  for sent in cleaned:\r\n",
        "      tokens = bigram_phraser[sent]\r\n",
        "      bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # run again to get trigrams\r\n",
        "  trigram = Phrases(bigramed_tokens, min_count=min_count, threshold=threshold, \r\n",
        "                    delimiter=b' ')\r\n",
        "  trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "  trigramed_tokens = []\r\n",
        "  for sent in bigramed_tokens:\r\n",
        "      tokens = trigram_phraser[sent]\r\n",
        "      trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # build a toy model to update with\r\n",
        "  model = Word2Vec(size=300, min_count=5)\r\n",
        "  model.build_vocab(trigramed_tokens)\r\n",
        "  total_examples = model.corpus_count\r\n",
        "\r\n",
        "  # add GloVe's vocabulary & weights\r\n",
        "  model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "  # train on our data\r\n",
        "  model.train(trigramed_tokens, total_examples=total_examples, epochs=model.epochs)\r\n",
        "  model_wv = model.wv\r\n",
        "  del model\r\n",
        "  return model_wv"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIMLCNVHdxID"
      },
      "source": [
        "ph_model = train_glove(source_type='school', source='phenomenology', glove_vectors=glove_vectors)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI3_J1HGnxd-",
        "outputId": "b2e6e402-c3af-4b9a-dd26-6ce3a88e4080"
      },
      "source": [
        "test_w2v(ph_model, pairs_to_try)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- magical (0.9949)\n",
            "- nervous (0.99348)\n",
            "- exhibitings (0.99332)\n",
            "- gestures (0.99329)\n",
            "- categorial (0.99328)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- potentiality (0.95779)\n",
            "- nature (0.94881)\n",
            "- authentic (0.93771)\n",
            "- existence (0.93201)\n",
            "- future (0.93043)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- contact (0.99394)\n",
            "- steps (0.99185)\n",
            "- journey (0.99121)\n",
            "- purposes (0.9898)\n",
            "- placing (0.9894)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- definition (0.97844)\n",
            "- phenomenology (0.97734)\n",
            "- infinity (0.97683)\n",
            "- constancy (0.97512)\n",
            "- modification (0.97508)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRp6eNmHeP01"
      },
      "source": [
        "Using the phenomenology vectors on some central terms of German idealism once again yields some pretty compelling results, except for where the words are rarely used by the phenomenologists. This is to be expected. Let's try the word vectors on some central terms of phenomenology."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWzc_fFGnXnF",
        "outputId": "826a5180-5840-4664-e073-84fed72b3114"
      },
      "source": [
        "pairs_to_try = [(['perception'], []),\r\n",
        "                (['dasein'], []),\r\n",
        "                (['consciousness'], []),\r\n",
        "                (['method'], []),]\r\n",
        "\r\n",
        "test_w2v(ph_model, pairs_to_try)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['perception']\tNegative - []\n",
            "- act (0.91805)\n",
            "- reality (0.91235)\n",
            "- knowledge (0.91063)\n",
            "- certainty (0.90632)\n",
            "- care (0.90189)\n",
            "\n",
            "Positive - ['dasein']\tNegative - []\n",
            "- itself (0.88474)\n",
            "- being (0.86611)\n",
            "- truth (0.84091)\n",
            "- consciousness (0.83831)\n",
            "- future (0.8306)\n",
            "\n",
            "Positive - ['consciousness']\tNegative - []\n",
            "- future (0.92622)\n",
            "- death (0.91877)\n",
            "- nature (0.90971)\n",
            "- potentiality (0.90915)\n",
            "- truth (0.90738)\n",
            "\n",
            "Positive - ['method']\tNegative - []\n",
            "- spirit (0.97481)\n",
            "- necessity (0.97436)\n",
            "- ontology (0.97354)\n",
            "- origin (0.9642)\n",
            "- process (0.95573)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZBBODD-neYi"
      },
      "source": [
        "These look pretty strong. Overall, the GloVe-traiend vectors seem to be an effective tool for revealing how a word is used by a school. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5UPKzWjrWMQ"
      },
      "source": [
        "#### Training on every school & author"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIREGokArZQK"
      },
      "source": [
        "To further explore this, we'll train w2v models in this way for each school and examine how each of them looks at the same word - 'philosophy.' We can use these in our future dashboard work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8_XI24ogh17",
        "outputId": "1acac61a-fe05-4a69-dac0-eb82dc9dfde1"
      },
      "source": [
        "w2v_dict = {}\r\n",
        "\r\n",
        "for school in df['school'].unique():\r\n",
        "  w2v_dict[school] = train_glove(school, glove_vectors=glove_vectors)\r\n",
        "  print(f'{school} completed')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "plato completed\n",
            "aristotle completed\n",
            "empiricism completed\n",
            "rationalism completed\n",
            "analytic completed\n",
            "continental completed\n",
            "phenomenology completed\n",
            "german_idealism completed\n",
            "communism completed\n",
            "capitalism completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euZktiqighpT",
        "outputId": "a5b4ceee-734c-4fe8-b2ab-d3ec6b7a5f6b"
      },
      "source": [
        "for school in df['school'].unique():\r\n",
        "  print(f'\\t{school.upper()}')\r\n",
        "  print('----------------------')\r\n",
        "  test_w2v(w2v_dict[school], [(['philosophy'], [])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPLATO\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- relief (0.94646)\n",
            "- springs (0.94496)\n",
            "- friendship (0.94237)\n",
            "- greece (0.93422)\n",
            "- regime (0.93311)\n",
            "\n",
            "\tARISTOTLE\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- respiration (0.91699)\n",
            "- oratory (0.87146)\n",
            "- shrillness (0.87085)\n",
            "- holders (0.86975)\n",
            "- memory (0.85944)\n",
            "\n",
            "\tEMPIRICISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- religion (0.93495)\n",
            "- mankind (0.92261)\n",
            "- doctrine (0.92259)\n",
            "- inquiry (0.9155)\n",
            "- faith (0.90897)\n",
            "\n",
            "\tRATIONALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- person (0.95782)\n",
            "- return (0.92869)\n",
            "- death (0.92862)\n",
            "- fall (0.92596)\n",
            "- public (0.9258)\n",
            "\n",
            "\tANALYTIC\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- philosophical (0.90815)\n",
            "- hahn (0.85215)\n",
            "- carnap (0.84692)\n",
            "- semantics (0.84195)\n",
            "- davidson (0.83312)\n",
            "\n",
            "\tCONTINENTAL\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- metaphysics (0.9689)\n",
            "- unreason (0.95789)\n",
            "- history (0.95635)\n",
            "- consciousness (0.94364)\n",
            "- proposition (0.93498)\n",
            "\n",
            "\tPHENOMENOLOGY\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- method (0.9159)\n",
            "- metaphysics (0.91034)\n",
            "- spirit (0.90868)\n",
            "- phenomenology (0.89536)\n",
            "- science (0.89134)\n",
            "\n",
            "\tGERMAN_IDEALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- science (0.87517)\n",
            "- metaphysics (0.86891)\n",
            "- method (0.81549)\n",
            "- pure reason (0.78349)\n",
            "- definitions (0.78128)\n",
            "\n",
            "\tCOMMUNISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- ministers (0.99902)\n",
            "- inches (0.999)\n",
            "- syndicates (0.99897)\n",
            "- divisions (0.99894)\n",
            "- along (0.99891)\n",
            "\n",
            "\tCAPITALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- queen (0.99179)\n",
            "- tartar (0.98992)\n",
            "- greek (0.98812)\n",
            "- treaties (0.98806)\n",
            "- uniform (0.98774)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmP9U0WrghgT"
      },
      "source": [
        "Interestingly, many of these top words align quite strongly with the school's general attitude towards philosophy. Continental thinkers mentioning unreason, analytic philosophers focusing on semantics, and phenomenologists associating philosophy with a method all track well. The ones that don't make sense are those that don't problematize the nature of philosophy to any great degree - capitalist thinkers aren't out there trying to discuss the nature of philosophy.\r\n",
        "\r\n",
        "We'd also like vectors trained for each individual author. We can use these in our dashboard to enable intra-school comparisons of authors and generally allow for more fine-grained data exploration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "V6KZckIvrDv8"
      },
      "source": [
        "#@title Glove Training Function Modified for Authors\n",
        "def train_glove_author(school, glove_vectors, threshold=10, stopwords=[],\n",
        "                min_count=20):\n",
        "  # isolate the relevant school\n",
        "  documents = df[df['author'] ==school]['gensim_tokenized']\n",
        "\n",
        "  # format the series to be used\n",
        "  stopwords = []\n",
        "\n",
        "  sentences = [sentence for sentence in documents]\n",
        "  cleaned = []\n",
        "  for sentence in sentences:\n",
        "    cleaned_sentence = [word.lower() for word in sentence]\n",
        "    cleaned_sentence = [word for word in sentence if word not in stopwords]\n",
        "    cleaned.append(cleaned_sentence)\n",
        "\n",
        "  # get bigrams\n",
        "  bigram = Phrases(cleaned, min_count=min_count, threshold=threshold, \n",
        "                   delimiter=b' ')\n",
        "  bigram_phraser = Phraser(bigram)\n",
        "\n",
        "  bigramed_tokens = []\n",
        "  for sent in cleaned:\n",
        "      tokens = bigram_phraser[sent]\n",
        "      bigramed_tokens.append(tokens)\n",
        "\n",
        "  # run again to get trigrams\n",
        "  trigram = Phrases(bigramed_tokens, min_count=min_count, threshold=threshold, \n",
        "                    delimiter=b' ')\n",
        "  trigram_phraser = Phraser(trigram)\n",
        "\n",
        "  trigramed_tokens = []\n",
        "  for sent in bigramed_tokens:\n",
        "      tokens = trigram_phraser[sent]\n",
        "      trigramed_tokens.append(tokens)\n",
        "\n",
        "  # build a toy model to update with\n",
        "  model = Word2Vec(size=300, min_count=5)\n",
        "  model.build_vocab(trigramed_tokens)\n",
        "  total_examples = model.corpus_count\n",
        "\n",
        "  # add GloVe's vocabulary & weights\n",
        "  model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\n",
        "\n",
        "  # train on our data\n",
        "  model.train(trigramed_tokens, total_examples=total_examples, epochs=model.epochs)\n",
        "  model_wv = model.wv\n",
        "  del model\n",
        "  return model_wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_IheEwhqWRi",
        "outputId": "a9ff4dae-0d01-419a-9a4f-40cf5a6a6c4c"
      },
      "source": [
        "for author in df['author'].unique():\r\n",
        "  w2v_dict[author] = train_glove_author(author, glove_vectors=glove_vectors)\r\n",
        "  print(f'{author} completed')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plato completed\n",
            "Aristotle completed\n",
            "Locke completed\n",
            "Hume completed\n",
            "Berkeley completed\n",
            "Spinoza completed\n",
            "Leibniz completed\n",
            "Descartes completed\n",
            "Malebranche completed\n",
            "Russell completed\n",
            "Moore completed\n",
            "Wittgenstein completed\n",
            "Lewis completed\n",
            "Quine completed\n",
            "Popper completed\n",
            "Kripke completed\n",
            "Foucault completed\n",
            "Derrida completed\n",
            "Deleuze completed\n",
            "Merleau-Ponty completed\n",
            "Husserl completed\n",
            "Heidegger completed\n",
            "Kant completed\n",
            "Fichte completed\n",
            "Hegel completed\n",
            "Marx completed\n",
            "Lenin completed\n",
            "Smith completed\n",
            "Ricardo completed\n",
            "Keynes completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av0A9Ohwsizl"
      },
      "source": [
        "With this finished - our next step is to train one on the entire corpus for use in classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaV6IV5esoW7"
      },
      "source": [
        "#### Building a Model for the full Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQu70aYwChpm"
      },
      "source": [
        "documents = df['gensim_tokenized']\r\n",
        "\r\n",
        "# format the series to be used\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "# get bigrams\r\n",
        "bigram = Phrases(cleaned, min_count=30, threshold=10, \r\n",
        "                  delimiter=b' ')\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "bigramed_tokens = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens = bigram_phraser[sent]\r\n",
        "    bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# run again to get trigrams\r\n",
        "trigram = Phrases(bigramed_tokens, min_count=30, threshold=10, \r\n",
        "                  delimiter=b' ')\r\n",
        "trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "trigramed_tokens = []\r\n",
        "for sent in bigramed_tokens:\r\n",
        "    tokens = trigram_phraser[sent]\r\n",
        "    trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "all_text_model = Word2Vec(size=300, min_count=5)\r\n",
        "all_text_model.build_vocab(trigramed_tokens)\r\n",
        "total_examples = all_text_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "all_text_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "all_text_model.train(trigramed_tokens, total_examples=total_examples, \r\n",
        "                     epochs=all_text_model.epochs)\r\n",
        "all_text_wv = all_text_model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wk3zbVgDbxl"
      },
      "source": [
        "As a test case, let's see how the philosophy thinks of itself as compared to how glove thinks of philosophy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GNhMEqvDnpN",
        "outputId": "b6271d35-e5b0-444d-d8ba-63bee3140904"
      },
      "source": [
        "for model in [1, 2]:\r\n",
        "  if model == 1:\r\n",
        "    print(f'\\tPHILOSOPHY CORPUS')\r\n",
        "    print('------------------------------------')\r\n",
        "    test_w2v(all_text_wv, [(['philosophy'], [])])\r\n",
        "  if model == 2:\r\n",
        "    print(f'\\tBASE GLOVE')\r\n",
        "    print('------------------------------------')\r\n",
        "    test_w2v(glove_vectors, [(['philosophy'], [])])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPHILOSOPHY CORPUS\n",
            "------------------------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- theology (0.8004)\n",
            "- metaphysics (0.77228)\n",
            "- religion (0.73747)\n",
            "- science (0.72425)\n",
            "- philosophical (0.71879)\n",
            "\n",
            "\tBASE GLOVE\n",
            "------------------------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- theology (0.88151)\n",
            "- philosophical (0.84362)\n",
            "- mathematics (0.83389)\n",
            "- psychology (0.82387)\n",
            "- sociology (0.81085)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeVE7mUFzlY"
      },
      "source": [
        "This sort of stands to reason - 'metaphysics' often has a different meaning outside of philosophical discussion, so it's not surprising to see it as the most changed term here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A24iStA_rtOf"
      },
      "source": [
        "#### Finalized exporting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUxG_7YlrvIP"
      },
      "source": [
        "All in all, things look good, so let's export the vectors so that they can be used in our neural networks and in our dash app. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg6rnhbD235n"
      },
      "source": [
        "# do not run these cells if you want to keep old w2v versions\r\n",
        "all_text_wv.save_word2vec_format('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/w2v_for_nn.bin')\r\n",
        "all_text_wv.save('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/w2v_for_nn.wordvectors')\r\n",
        "\r\n",
        "for source in w2v_dict.keys():\r\n",
        "  w2v_dict[source].save(f'/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/{source}_w2v.wordvectors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyhr9Cqpm3p4"
      },
      "source": [
        "#### Exporting for Only New Texts\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT2C31yemkFO"
      },
      "source": [
        "# use this cell to build the newest author/text/school in the corpus, then export in the cell below\r\n",
        "epictetus_wv = train_glove(source_type='author', source='Epictetus', glove_vectors=glove_vectors)\r\n",
        "stoicism_wv = train_glove(source_type='school', source='stoicism', glove_vectors=glove_vectors)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgi2_RYaoJsQ",
        "outputId": "3898e4b6-261e-4725-8f88-49e88f63099b"
      },
      "source": [
        "epictetus_wv.most_similar('good')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('be', 0.9994500279426575),\n",
              " ('to', 0.9994434714317322),\n",
              " ('you', 0.9994379281997681),\n",
              " ('and', 0.9994164705276489),\n",
              " ('it', 0.9994126558303833),\n",
              " ('the', 0.999406099319458),\n",
              " ('that', 0.9993921518325806),\n",
              " ('is', 0.9993898868560791),\n",
              " ('he', 0.9993888139724731),\n",
              " ('of', 0.9993849396705627)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYziok8npUj"
      },
      "source": [
        "epictetus_wv.save('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/Epictetus_w2v.wordvectors')\r\n",
        "stoicism_wv.save('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/Stoiticism_w2v.wordvectors')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0wn9YYoGXel"
      },
      "source": [
        "And that's it! See our other notebooks for more of the modeling work. "
      ]
    }
  ]
}
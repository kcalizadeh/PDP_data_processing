{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v_for_imports.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fRqjeaLlmZHe",
        "_7J7_WOz2VK8"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMGN81Ej7oHVWyiFvn15K9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcalizadeh/PDP_data_processing/blob/master/w2v_for_imports.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxG_6U1lGEa"
      },
      "source": [
        "### Imports and Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRT6BjwbiM8n",
        "outputId": "aee065d5-dadf-44f0-a6b8-e503c9933eca"
      },
      "source": [
        "# this cell mounts drive, sets the correct directory, then imports all functions\n",
        "# and relevant libraries via the functions.py file\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/gdrive',force_remount=True)\n",
        "\n",
        "drive_path = '/gdrive/MyDrive/Colab_Projects/Phil_NLP'\n",
        "\n",
        "sys.path.append(drive_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_FSrsN8l0J4"
      },
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "\n",
        "# a function for quickly testing w2v models\n",
        "def test_w2v(model, pairs):\n",
        "  for (pos, neg) in pairs:\n",
        "    math_result = model.most_similar(positive=pos, negative=neg)\n",
        "    print(f'Positive - {pos}\\tNegative - {neg}')\n",
        "    [print(f\"- {result[0]} ({round(result[1],5)})\") for result in math_result[:5]]\n",
        "    print()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRqjeaLlmZHe"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "PHxy7346l3hZ",
        "outputId": "f6d5797e-8b91-40a0-edef-7f71b21dc2c2"
      },
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/phil_nlp.csv')\n",
        "\n",
        "df.sample(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>school</th>\n",
              "      <th>sentence_spacy</th>\n",
              "      <th>sentence_str</th>\n",
              "      <th>sentence_length</th>\n",
              "      <th>sentence_lowered</th>\n",
              "      <th>tokenized_txt</th>\n",
              "      <th>lemmatized_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13786</th>\n",
              "      <td>Plato - Complete Works</td>\n",
              "      <td>Plato</td>\n",
              "      <td>plato</td>\n",
              "      <td>So, since I am your friend, I would never dare...</td>\n",
              "      <td>So, since I am your friend, I would never dare...</td>\n",
              "      <td>89</td>\n",
              "      <td>so, since i am your friend, i would never dare...</td>\n",
              "      <td>['so', 'since', 'am', 'your', 'friend', 'would...</td>\n",
              "      <td>so , since -PRON- be -PRON- friend , -PRON- w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294204</th>\n",
              "      <td>Capital</td>\n",
              "      <td>Marx</td>\n",
              "      <td>communism</td>\n",
              "      <td>When the system attained to a certain degree o...</td>\n",
              "      <td>When the system attained to a certain degree o...</td>\n",
              "      <td>254</td>\n",
              "      <td>when the system attained to a certain degree o...</td>\n",
              "      <td>['when', 'the', 'system', 'attained', 'to', 'c...</td>\n",
              "      <td>when the system attain to a certain degree of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316954</th>\n",
              "      <td>The Wealth Of Nations</td>\n",
              "      <td>Smith</td>\n",
              "      <td>capitalism</td>\n",
              "      <td>In those corrupted governments, where there is...</td>\n",
              "      <td>In those corrupted governments, where there is...</td>\n",
              "      <td>173</td>\n",
              "      <td>in those corrupted governments, where there is...</td>\n",
              "      <td>['in', 'those', 'corrupted', 'governments', 'w...</td>\n",
              "      <td>in those corrupt government , where there be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295890</th>\n",
              "      <td>Capital</td>\n",
              "      <td>Marx</td>\n",
              "      <td>communism</td>\n",
              "      <td>improvements succeeded each other so rapidly, ...</td>\n",
              "      <td>improvements succeeded each other so rapidly, ...</td>\n",
              "      <td>189</td>\n",
              "      <td>improvements succeeded each other so rapidly, ...</td>\n",
              "      <td>['improvements', 'succeeded', 'each', 'other',...</td>\n",
              "      <td>improvement succeed each other so rapidly , t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226983</th>\n",
              "      <td>The Crisis Of The European Sciences And Phenom...</td>\n",
              "      <td>Husserl</td>\n",
              "      <td>phenomenology</td>\n",
              "      <td>Our focus on the world of perception (and it i...</td>\n",
              "      <td>Our focus on the world of perception (and it i...</td>\n",
              "      <td>242</td>\n",
              "      <td>our focus on the world of perception (and it i...</td>\n",
              "      <td>['our', 'focus', 'on', 'the', 'world', 'of', '...</td>\n",
              "      <td>-PRON- focus on the world of perception ( and...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    title  ...                                     lemmatized_str\n",
              "13786                              Plato - Complete Works  ...   so , since -PRON- be -PRON- friend , -PRON- w...\n",
              "294204                                            Capital  ...   when the system attain to a certain degree of...\n",
              "316954                              The Wealth Of Nations  ...   in those corrupt government , where there be ...\n",
              "295890                                            Capital  ...   improvement succeed each other so rapidly , t...\n",
              "226983  The Crisis Of The European Sciences And Phenom...  ...   -PRON- focus on the world of perception ( and...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR-d0hRI7hmQ"
      },
      "source": [
        "# using gensim's built-in tokenizer \r\n",
        "df['gensim_tokenized'] = df['sentence_str'].map(lambda x: simple_preprocess(x.lower(),deacc=True,\r\n",
        "                                                        max_len=100))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-mTBCSDojS1",
        "outputId": "4b820c8b-ac12-40a8-8c9f-8eedcd62a735"
      },
      "source": [
        "# check how it worked\r\n",
        "print(df.iloc[290646]['sentence_str'])\r\n",
        "df['gensim_tokenized'][290646]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A spider conducts operations that resemble those of a weaver, and a bee puts to shame many an architect in the construction of her cells.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spider',\n",
              " 'conducts',\n",
              " 'operations',\n",
              " 'that',\n",
              " 'resemble',\n",
              " 'those',\n",
              " 'of',\n",
              " 'weaver',\n",
              " 'and',\n",
              " 'bee',\n",
              " 'puts',\n",
              " 'to',\n",
              " 'shame',\n",
              " 'many',\n",
              " 'an',\n",
              " 'architect',\n",
              " 'in',\n",
              " 'the',\n",
              " 'construction',\n",
              " 'of',\n",
              " 'her',\n",
              " 'cells']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KxV2JTIoyxE"
      },
      "source": [
        "Hmm, an interesting observation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5ZnODM2V_P"
      },
      "source": [
        "### Transfer Learning with GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAOaelq2WIr"
      },
      "source": [
        "We'll import GloVe vectors as w2v, then use those as a base from which to train new vectors that are tuned to our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcFZMRT82WRC"
      },
      "source": [
        "# load the vectors. other vector sizes were used but yielded generally less sensible models\r\n",
        "glove_file = datapath('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/glove.6B.50d.txt')\r\n",
        "tmp_file = get_tmpfile(\"test_word2vec.txt\")\r\n",
        "\r\n",
        "_ = glove2word2vec(glove_file, tmp_file)\r\n",
        "\r\n",
        "glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hPyZ2aHugg"
      },
      "source": [
        "pairs_to_try = [(['law', 'moral'], []),\r\n",
        "                (['self', 'consciousness'], []),\r\n",
        "                (['dialectic'], []),\r\n",
        "                (['logic'], []),\r\n",
        "]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjdDWtd72Tli",
        "outputId": "7f96d706-ea38-4672-9000-a800ee937173"
      },
      "source": [
        "# check out how GloVe works on our test pairs\r\n",
        "test_w2v(glove_vectors, pairs_to_try)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- morality (0.82654)\n",
            "- legal (0.82652)\n",
            "- laws (0.81529)\n",
            "- constitutional (0.80616)\n",
            "- fundamental (0.80217)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- sense (0.83446)\n",
            "- mind (0.79755)\n",
            "- vision (0.78202)\n",
            "- belief (0.78031)\n",
            "- life (0.77984)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- hegelian (0.88376)\n",
            "- dialectical (0.83417)\n",
            "- dialectics (0.80672)\n",
            "- materialist (0.77674)\n",
            "- metaphysics (0.77488)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- reasoning (0.81405)\n",
            "- intuitionistic (0.76531)\n",
            "- concepts (0.75831)\n",
            "- logical (0.75604)\n",
            "- theory (0.75026)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFyauZoo7KW"
      },
      "source": [
        "Now we want these to be trained on our actual philosophical texts - that way we can see how different thinkers use different words and potentially use the vectors for classification.\r\n",
        "\r\n",
        "So in the cells below we train the existing GloVe model on on the German Idealist texts as a test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVDl6FwMmwp9"
      },
      "source": [
        "#### German Idealism Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSOA8Zz2o7vX"
      },
      "source": [
        "# isolate the relevant school\r\n",
        "documents = df[df['school'] == 'german_idealism']['gensim_tokenized']\r\n",
        "\r\n",
        "# format the series to be used\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "# get bigrams\r\n",
        "bigram = Phrases(cleaned, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "bigramed_tokens = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens = bigram_phraser[sent]\r\n",
        "    bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# run again to get trigrams\r\n",
        "trigram = Phrases(bigramed_tokens, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "trigramed_tokens = []\r\n",
        "for sent in bigramed_tokens:\r\n",
        "    tokens = trigram_phraser[sent]\r\n",
        "    trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "base_model = Word2Vec(size=300, min_count=5)\r\n",
        "base_model.build_vocab(trigramed_tokens)\r\n",
        "total_examples = base_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "base_model.train(trigramed_tokens, total_examples=total_examples, epochs=base_model.epochs)\r\n",
        "base_model_wv = base_model.wv\r\n",
        "del base_model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0FjXahncQya",
        "outputId": "305641d2-b529-438b-e6ee-9a43b0c244ec"
      },
      "source": [
        "test_w2v(base_model_wv, pairs_to_try)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- rule (0.82387)\n",
            "- moral law (0.80709)\n",
            "- freedom (0.80685)\n",
            "- causality (0.79339)\n",
            "- happiness (0.79102)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- self consciousness (0.92469)\n",
            "- essence (0.87722)\n",
            "- immediacy (0.87623)\n",
            "- objectivity (0.87523)\n",
            "- negativity (0.87135)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- method (0.93279)\n",
            "- doctrine (0.9254)\n",
            "- antinomy (0.92351)\n",
            "- deduction (0.91393)\n",
            "- exposition (0.89874)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- pure reason (0.85429)\n",
            "- doctrine (0.84595)\n",
            "- method (0.838)\n",
            "- science (0.82746)\n",
            "- dialectic (0.81556)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR3MnHFbcl7d"
      },
      "source": [
        "These seem to reflect the true uses of words in German Idealism. 'Self' + 'consciousness' is rightly associated with 'self consciousness' and 'moral' + 'law' with 'moral law'. It even identifies the German Idealist tendency to unify logic and metaphysics. \r\n",
        "\r\n",
        "These vectors can be fairly said to reflect how german idealists use these terms. Moreover, they are significantly different than the original GloVe model, which indicates that there was real learning going on here.\r\n",
        "\r\n",
        "For comparison, let's check these same terms, but as used by Phenomenologists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPdzhIk0m0Qz"
      },
      "source": [
        "#### Phenomenology Comparision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s1Jkkc7dqk9"
      },
      "source": [
        "def train_glove(source_type, source, glove_vectors, threshold=10, stopwords=[],\r\n",
        "                min_count=20):\r\n",
        "  # isolate the relevant school\r\n",
        "  documents = df[df[source_type] == source]['gensim_tokenized']\r\n",
        "\r\n",
        "  # format the series to be used\r\n",
        "  stopwords = []\r\n",
        "\r\n",
        "  sentences = [sentence for sentence in documents]\r\n",
        "  cleaned = []\r\n",
        "  for sentence in sentences:\r\n",
        "    cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "    cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "    cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "  # get bigrams\r\n",
        "  bigram = Phrases(cleaned, min_count=min_count, threshold=threshold, \r\n",
        "                   delimiter=b' ')\r\n",
        "  bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "  bigramed_tokens = []\r\n",
        "  for sent in cleaned:\r\n",
        "      tokens = bigram_phraser[sent]\r\n",
        "      bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # run again to get trigrams\r\n",
        "  trigram = Phrases(bigramed_tokens, min_count=min_count, threshold=threshold, \r\n",
        "                    delimiter=b' ')\r\n",
        "  trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "  trigramed_tokens = []\r\n",
        "  for sent in bigramed_tokens:\r\n",
        "      tokens = trigram_phraser[sent]\r\n",
        "      trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # build a toy model to update with\r\n",
        "  model = Word2Vec(size=300, min_count=5)\r\n",
        "  model.build_vocab(trigramed_tokens)\r\n",
        "  total_examples = model.corpus_count\r\n",
        "\r\n",
        "  # add GloVe's vocabulary & weights\r\n",
        "  model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "  # train on our data\r\n",
        "  model.train(trigramed_tokens, total_examples=total_examples, epochs=model.epochs)\r\n",
        "  model_wv = model.wv\r\n",
        "  del model\r\n",
        "  return model_wv"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIMLCNVHdxID"
      },
      "source": [
        "ph_model = train_glove(source_type='school', source='phenomenology', glove_vectors=glove_vectors)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI3_J1HGnxd-",
        "outputId": "54d33179-2b7d-45af-bf00-885de145fb8d"
      },
      "source": [
        "test_w2v(ph_model, pairs_to_try)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- ideality (0.99645)\n",
            "- unfolding (0.99551)\n",
            "- ab (0.99506)\n",
            "- class (0.99357)\n",
            "- intentionality (0.99174)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- certainty (0.95246)\n",
            "- potentiality (0.94868)\n",
            "- existence (0.94029)\n",
            "- nature (0.93831)\n",
            "- authentic (0.93513)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- danger (0.98913)\n",
            "- room (0.9881)\n",
            "- shadow (0.9869)\n",
            "- furthermore (0.9869)\n",
            "- publicness (0.98683)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- definition (0.98022)\n",
            "- rpretation (0.9788)\n",
            "- type (0.9781)\n",
            "- transcendence (0.97677)\n",
            "- dispensation (0.97635)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRp6eNmHeP01"
      },
      "source": [
        "Using the phenomenology vectors on some central terms of German idealism once again yields some pretty compelling results, except for where the words are rarely used by the phenomenologists. This is to be expected. Let's try the word vectors on some central terms of phenomenology."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWzc_fFGnXnF",
        "outputId": "afb3d1b6-64e1-4954-af5c-39b3f571c9ae"
      },
      "source": [
        "pairs_to_try = [(['perception'], []),\r\n",
        "                (['dasein'], []),\r\n",
        "                (['consciousness'], []),\r\n",
        "                (['method'], []),]\r\n",
        "\r\n",
        "test_w2v(ph_model, pairs_to_try)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['perception']\tNegative - []\n",
            "- act (0.9331)\n",
            "- death (0.93105)\n",
            "- representation (0.93053)\n",
            "- movement (0.92326)\n",
            "- synthesis (0.91799)\n",
            "\n",
            "Positive - ['dasein']\tNegative - []\n",
            "- being (0.8908)\n",
            "- truth (0.87897)\n",
            "- itself (0.87614)\n",
            "- future (0.85072)\n",
            "- consciousness (0.83773)\n",
            "\n",
            "Positive - ['consciousness']\tNegative - []\n",
            "- movement (0.93418)\n",
            "- representation (0.91543)\n",
            "- body (0.90922)\n",
            "- existence (0.90389)\n",
            "- future (0.90297)\n",
            "\n",
            "Positive - ['method']\tNegative - []\n",
            "- spirit (0.97806)\n",
            "- necessity (0.95537)\n",
            "- phenomenology (0.95236)\n",
            "- ontology (0.95015)\n",
            "- history (0.95015)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5UPKzWjrWMQ"
      },
      "source": [
        "#### Training on every school & author"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIREGokArZQK"
      },
      "source": [
        "To further explore this, we'll train w2v models in this way for each school and examine how each of them looks at the same word - 'philosophy.' We can use these in our future dashboard work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8_XI24ogh17",
        "outputId": "b0b6a4d3-1c80-41ed-d506-ddc2dfa851d4"
      },
      "source": [
        "w2v_dict = {}\r\n",
        "\r\n",
        "for school in df['school'].unique():\r\n",
        "  w2v_dict[school] = train_glove('school', school, glove_vectors=glove_vectors)\r\n",
        "  print(f'{school} completed')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "plato completed\n",
            "aristotle completed\n",
            "empiricism completed\n",
            "rationalism completed\n",
            "analytic completed\n",
            "continental completed\n",
            "phenomenology completed\n",
            "german_idealism completed\n",
            "communism completed\n",
            "capitalism completed\n",
            "stoicism completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euZktiqighpT",
        "outputId": "7d393b42-0fa1-4344-e504-f5cfe7b046ba"
      },
      "source": [
        "for school in df['school'].unique():\r\n",
        "  print(f'\\t{school.upper()}')\r\n",
        "  print('----------------------')\r\n",
        "  test_w2v(w2v_dict[school], [(['philosophy'], [])])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPLATO\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- exploits (0.94807)\n",
            "- every kind (0.94536)\n",
            "- nourishment (0.9443)\n",
            "- construction (0.9407)\n",
            "- waves (0.93759)\n",
            "\n",
            "\tARISTOTLE\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- practice (0.90368)\n",
            "- affairs (0.89227)\n",
            "- writer (0.8858)\n",
            "- mortals (0.88398)\n",
            "- philosophical (0.87431)\n",
            "\n",
            "\tEMPIRICISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- subtle (0.91686)\n",
            "- practice (0.913)\n",
            "- religion (0.91133)\n",
            "- ethics (0.88737)\n",
            "- popular (0.87861)\n",
            "\n",
            "\tRATIONALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- name (0.93098)\n",
            "- prince (0.92751)\n",
            "- territory (0.92547)\n",
            "- intention (0.92448)\n",
            "- doctrine (0.92422)\n",
            "\n",
            "\tANALYTIC\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- philosophical (0.88962)\n",
            "- semantics (0.85898)\n",
            "- symbolism (0.83177)\n",
            "- quotation (0.82118)\n",
            "- modern (0.81951)\n",
            "\n",
            "\tCONTINENTAL\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- history (0.95461)\n",
            "- notion (0.94806)\n",
            "- unreason (0.94588)\n",
            "- metaphysics (0.93707)\n",
            "- capitalism (0.92847)\n",
            "\n",
            "\tPHENOMENOLOGY\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- spirit (0.90423)\n",
            "- metaphysics (0.89827)\n",
            "- phenomenology (0.89371)\n",
            "- punctuality (0.89011)\n",
            "- science (0.88829)\n",
            "\n",
            "\tGERMAN_IDEALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- science (0.87732)\n",
            "- metaphysics (0.85916)\n",
            "- mathematics (0.8103)\n",
            "- method (0.79468)\n",
            "- critique (0.78717)\n",
            "\n",
            "\tCOMMUNISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- preliminary (0.99842)\n",
            "- employing (0.9984)\n",
            "- abolishing (0.99828)\n",
            "- patriarchal (0.99827)\n",
            "- combines (0.99811)\n",
            "\n",
            "\tCAPITALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- italy (0.99112)\n",
            "- thedominions (0.99071)\n",
            "- loans (0.99042)\n",
            "- continent (0.98852)\n",
            "- skins (0.98764)\n",
            "\n",
            "\tSTOICISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- upon (0.9999)\n",
            "- but (0.99988)\n",
            "- me (0.99988)\n",
            "- let (0.99988)\n",
            "- thou hast (0.99988)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmP9U0WrghgT"
      },
      "source": [
        "Interestingly, many of these top words align quite strongly with the school's general attitude towards philosophy. Continental thinkers mentioning unreason, analytic philosophers focusing on semantics, and phenomenologists associating philosophy with a method all track well. The ones that don't make sense are those that don't problematize the nature of philosophy to any great degree - capitalist thinkers aren't out there trying to discuss the nature of philosophy.\r\n",
        "\r\n",
        "We'd also like vectors trained for each individual author. We can use these in our dashboard to enable intra-school comparisons of authors and generally allow for more fine-grained data exploration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "V6KZckIvrDv8"
      },
      "source": [
        "#@title Glove Training Function Modified for Authors\n",
        "def train_glove_author(school, glove_vectors, threshold=10, stopwords=[],\n",
        "                min_count=20):\n",
        "  # isolate the relevant school\n",
        "  documents = df[df['author'] ==school]['gensim_tokenized']\n",
        "\n",
        "  # format the series to be used\n",
        "  stopwords = []\n",
        "\n",
        "  sentences = [sentence for sentence in documents]\n",
        "  cleaned = []\n",
        "  for sentence in sentences:\n",
        "    cleaned_sentence = [word.lower() for word in sentence]\n",
        "    cleaned_sentence = [word for word in sentence if word not in stopwords]\n",
        "    cleaned.append(cleaned_sentence)\n",
        "\n",
        "  # get bigrams\n",
        "  bigram = Phrases(cleaned, min_count=min_count, threshold=threshold, \n",
        "                   delimiter=b' ')\n",
        "  bigram_phraser = Phraser(bigram)\n",
        "\n",
        "  bigramed_tokens = []\n",
        "  for sent in cleaned:\n",
        "      tokens = bigram_phraser[sent]\n",
        "      bigramed_tokens.append(tokens)\n",
        "\n",
        "  # run again to get trigrams\n",
        "  trigram = Phrases(bigramed_tokens, min_count=min_count, threshold=threshold, \n",
        "                    delimiter=b' ')\n",
        "  trigram_phraser = Phraser(trigram)\n",
        "\n",
        "  trigramed_tokens = []\n",
        "  for sent in bigramed_tokens:\n",
        "      tokens = trigram_phraser[sent]\n",
        "      trigramed_tokens.append(tokens)\n",
        "\n",
        "  # build a toy model to update with\n",
        "  model = Word2Vec(size=300, min_count=5)\n",
        "  model.build_vocab(trigramed_tokens)\n",
        "  total_examples = model.corpus_count\n",
        "\n",
        "  # add GloVe's vocabulary & weights\n",
        "  model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\n",
        "\n",
        "  # train on our data\n",
        "  model.train(trigramed_tokens, total_examples=total_examples, epochs=model.epochs)\n",
        "  model_wv = model.wv\n",
        "  del model\n",
        "  return model_wv"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_IheEwhqWRi",
        "outputId": "5f67e7da-efc3-4ff4-f0bd-97b65fb7706e"
      },
      "source": [
        "for author in df['author'].unique():\r\n",
        "  w2v_dict[author] = train_glove('author', author, glove_vectors=glove_vectors)\r\n",
        "  print(f'{author} completed')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plato completed\n",
            "Aristotle completed\n",
            "Locke completed\n",
            "Hume completed\n",
            "Berkeley completed\n",
            "Spinoza completed\n",
            "Leibniz completed\n",
            "Descartes completed\n",
            "Malebranche completed\n",
            "Russell completed\n",
            "Moore completed\n",
            "Wittgenstein completed\n",
            "Lewis completed\n",
            "Quine completed\n",
            "Popper completed\n",
            "Kripke completed\n",
            "Foucault completed\n",
            "Derrida completed\n",
            "Deleuze completed\n",
            "Merleau-Ponty completed\n",
            "Husserl completed\n",
            "Heidegger completed\n",
            "Kant completed\n",
            "Fichte completed\n",
            "Hegel completed\n",
            "Marx completed\n",
            "Lenin completed\n",
            "Smith completed\n",
            "Ricardo completed\n",
            "Keynes completed\n",
            "Epictetus completed\n",
            "Marcus Aurelius completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av0A9Ohwsizl"
      },
      "source": [
        "With this finished - our next step is to train one on the entire corpus for use in classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaV6IV5esoW7"
      },
      "source": [
        "#### Building a Model for the full Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQu70aYwChpm"
      },
      "source": [
        "documents = df['gensim_tokenized']\r\n",
        "\r\n",
        "# format the series to be used\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "# get bigrams\r\n",
        "bigram = Phrases(cleaned, min_count=30, threshold=10, \r\n",
        "                  delimiter=b' ')\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "bigramed_tokens = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens = bigram_phraser[sent]\r\n",
        "    bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# run again to get trigrams\r\n",
        "trigram = Phrases(bigramed_tokens, min_count=30, threshold=10, \r\n",
        "                  delimiter=b' ')\r\n",
        "trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "trigramed_tokens = []\r\n",
        "for sent in bigramed_tokens:\r\n",
        "    tokens = trigram_phraser[sent]\r\n",
        "    trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "all_text_model = Word2Vec(size=300, min_count=5)\r\n",
        "all_text_model.build_vocab(trigramed_tokens)\r\n",
        "total_examples = all_text_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "all_text_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "all_text_model.train(trigramed_tokens, total_examples=total_examples, \r\n",
        "                     epochs=all_text_model.epochs)\r\n",
        "all_text_wv = all_text_model.wv"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wk3zbVgDbxl"
      },
      "source": [
        "As a test case, let's see how the philosophy thinks of itself as compared to how glove thinks of philosophy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GNhMEqvDnpN",
        "outputId": "8a795d99-cf0c-46a3-8973-e9b567fc3595"
      },
      "source": [
        "for model in [1, 2]:\r\n",
        "  if model == 1:\r\n",
        "    print(f'\\tPHILOSOPHY CORPUS')\r\n",
        "    print('------------------------------------')\r\n",
        "    test_w2v(all_text_wv, [(['philosophy'], [])])\r\n",
        "  if model == 2:\r\n",
        "    print(f'\\tBASE GLOVE')\r\n",
        "    print('------------------------------------')\r\n",
        "    test_w2v(glove_vectors, [(['philosophy'], [])])\r\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPHILOSOPHY CORPUS\n",
            "------------------------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- metaphysics (0.7847)\n",
            "- theology (0.77706)\n",
            "- religion (0.72583)\n",
            "- transcendental philosophy (0.71516)\n",
            "- philosophical (0.71454)\n",
            "\n",
            "\tBASE GLOVE\n",
            "------------------------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- theology (0.88151)\n",
            "- philosophical (0.84362)\n",
            "- mathematics (0.83389)\n",
            "- psychology (0.82387)\n",
            "- sociology (0.81085)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeVE7mUFzlY"
      },
      "source": [
        "This sort of stands to reason - 'metaphysics' often has a different meaning outside of philosophical discussion, so it's not surprising to see it as the most changed term here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyhr9Cqpm3p4"
      },
      "source": [
        "#### Exporting for Only New Texts\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3tZfuM9j192"
      },
      "source": [
        "w2v_dict['Wittgenstein'].save('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/Wittgenstein_w2v.wordvectors')\r\n",
        "w2v_dict['analytic'].save('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/analytic_w2v.wordvectors')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT2C31yemkFO"
      },
      "source": [
        "# use this cell to build the newest author/text/school in the corpus, then export in the cell below\r\n",
        "epictetus_wv = train_glove(source_type='author', source='Epictetus', glove_vectors=glove_vectors)\r\n",
        "aurelius_wv = train_glove(source_type='author', source='Marcus Aurelius', glove_vectors=glove_vectors)\r\n",
        "stoicism_wv = train_glove(source_type='school', source='stoicism', glove_vectors=glove_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgi2_RYaoJsQ",
        "outputId": "809a552b-66fd-44b4-a190-21cbf07dd7bb"
      },
      "source": [
        "# test out new model\r\n",
        "stoicism_wv.most_similar('man')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('without', 0.9999505877494812),\n",
              " ('thou art', 0.9999483823776245),\n",
              " ('anything', 0.999948263168335),\n",
              " ('neither', 0.9999476671218872),\n",
              " ('an', 0.999946117401123),\n",
              " ('but', 0.9999449253082275),\n",
              " ('him', 0.9999449253082275),\n",
              " ('thou hast', 0.9999435544013977),\n",
              " ('able', 0.9999430179595947),\n",
              " ('before', 0.9999427795410156)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYziok8npUj"
      },
      "source": [
        "epictetus_wv.save('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/Epictetus_w2v.wordvectors')\r\n",
        "stoicism_wv.save('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/Stoiticism_w2v.wordvectors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A24iStA_rtOf"
      },
      "source": [
        "#### Finalized exporting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUxG_7YlrvIP"
      },
      "source": [
        "All in all, things look good, so let's export the vectors so that they can be used in our neural networks and in our dash app. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg6rnhbD235n"
      },
      "source": [
        "# do not run these cells if you want to keep old w2v versions\r\n",
        "all_text_wv.save_word2vec_format('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/w2v_for_nn.bin')\r\n",
        "all_text_wv.save('/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/w2v_for_nn.wordvectors')\r\n",
        "\r\n",
        "for source in w2v_dict.keys():\r\n",
        "  w2v_dict[source].save(f'/gdrive/MyDrive/Colab_Projects/philosophy_data_project/w2v_models/{source}_w2v.wordvectors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0wn9YYoGXel"
      },
      "source": [
        "And that's it! See our other notebooks for more of the modeling work. "
      ]
    }
  ]
}